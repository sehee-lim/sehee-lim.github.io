---
layout: post
title: "ELECTRA: Pre-Training Text Encoders as Discriminators Rather Than Generators"
category: "NLP"
date: 2023-10-31
---

<br>


ELECTRA 모델은 Transformer 기반 모델들에 비해 성능이 높고 학습 시간이 빠르다. BERT와 같은 대부분의 모델은 pre-training 중에 MLM(Masked Language Modeling) 작업을 수행한다. 즉 일정 비율의 token을 [MASK] token으로 바꾸고 원래 token으로 예측한다. 그러나 ELECTRA 모델은 그와 다르게 접근한다. ELECTRA 모델은 원문에서 임의의 token을 다른 token으로 교체하고 이 token이 **원래 token인지 아니면 교체된 token인지 예측**한다. BERT 모델은 [MASK] token의 위치에서만 loss를 계산하지만 ELECTRA 모델은 모든 token에 대해서 loss를 계산하기 때문에 학습이 훨씬 **효율적**이다.

BERT 모델은 pre-training하는 동안 일부를 [MASK] token으로 바꿔서 학습한다. 하지만 실제 작업을 수행할 때에는 이 [MASK] token을 접하지 않는다. 즉 pre-training과 실제 작업 수행 사이에 일관성이 없다. ALBERT 모델은 [MASK] token의 사용을 회피하여 이 불일치 문제를 해결할 수 있다.

![Untitled](/assets/ELECTRA%20Pre-Training%20Text%20Encoders%20as%20Discriminato%20f10f4341ebab44d597730ecc4fb4260d/Untitled.png)

ALBERT 모델은 다른 모델에 비해 빠르게 성능이 향상된다. 그리고 같은 조건에서 MLM 작업을 사용한 모델에 비해서 성능이 좋다.

<br>
<br>

**Replaced Token Detection**

Generator는 학습 데이터에서 일부 token을 손상시키는 데 사용된다. 즉 일부 token을 선택하고 그 token을 다른 token으로 대체한다. Discriminator는 각 token이 원래 token인지 아니면 generator에 의해 대체된 token인지 분류한다. Pre-training 이후에는 generator를 제거하고 downstream 작업에 대해서 discriminator만 fine-tuning한다.

![Untitled](/assets/ELECTRA%20Pre-Training%20Text%20Encoders%20as%20Discriminato%20f10f4341ebab44d597730ecc4fb4260d/Untitled%201.png)

Input token의 sequence를 $\bold{x} = [x_1, \cdots, x_n]$라 하고 여기에 문맥 정보를 입힌 벡터를 $h(\bold{x})$라고 하자. 그렇다면 각 후보 token에 대해서 generator가 출력하는 확률은 다음과 같다.

$$
p_G (x_t \vert \bold{x}) = \exp (e(x_t)^T h_G(\bold{x})_t) / \sum_x \exp (e(x')^T h_G(\bold{x})_t)
$$

여기서 $x_t$는 [MASK] token 위치에 대체될 수 있는 후보 token이다. 그리고 $e$는 token embedding을 나타낸다. 즉 $e(x_t)^T h_G(\bold{x})_t$는 embedding 벡터와 hidden state 벡터 사이의 유사성을 측정하는 내적이다. 이 값이 크면 해당 token이 현재 문맥에서 적절하다는 것을 의미한다.

그리고 discriminator는 $x_t$가 실제 token인지 아니면 대체된 token인지 판별하는 sigmoid layer를 거친다. 이 판별은 내적으로 통해 얻어진 점수를 sigmoid 함수를 통해 확률 값으로 변환하여 수행된다. 즉 출력 값은 $x_t$가 실제 token일 확률을 나타낸다.

$$
D(\bold{x}, t) = \text{sigmoid} (w^T h_D(\bold{x})_t)
$$


<br>
<br>


**모델 학습 과정**

1. Input이 총 $n$개일 때 1부터 $n$ 중에서 [MASK] token을 적용할 위치를 random하게 sampling한다.

$$
m_i \sim unif\{1, n\} \quad \text{for} \ \ i = 1 \ \ \text{to} \ \ k
$$

1. 뽑힌 위치에 있는 입력 token을 [MASK]로 바꾼다.

![Untitled](/assets/ELECTRA%20Pre-Training%20Text%20Encoders%20as%20Discriminato%20f10f4341ebab44d597730ecc4fb4260d/Untitled%202.png)

1. [MASK] token이 존재하는 input에 대해서 generator는 원래 token이 무엇인지 예측할 수 있도록 학습한다.
2. Discriminator는 각 token이 generator에 의해 대체된 token인지 원래 token인지 구별할 수 있도록 학습한다. Discriminator가 학습하는 데이터는 다음과 같다.

$$
\hat{x}_i \sim p_G (x_i \vert \bold{x}^{\text{masked}}) \quad for \ \ i \in m
$$

![Untitled](/assets/ELECTRA%20Pre-Training%20Text%20Encoders%20as%20Discriminato%20f10f4341ebab44d597730ecc4fb4260d/Untitled%203.png)

이것은 generator와 discriminator를 jointly하게 학습시키는 것이다. 그런데 generator만 $n$번 학습시키고 disciminator를 generator의 가중치로 initialization한 후 disciminator를 학습시키는 방법도 있다. 결과적으로 generator와 discriminator를 jointly하게 학습시키는 것이 성능이 더 좋았다.

![Untitled](/assets/ELECTRA%20Pre-Training%20Text%20Encoders%20as%20Discriminato%20f10f4341ebab44d597730ecc4fb4260d/Untitled%204.png)


<br>
<br>


**Generator의 크기**

만약 generator와 discriminator의 크기를 동일하게 한다면 ELECTRA 모델은 단순히 MLM 작업만 시키는 모델에 비해 두 배 더 많은 computing을 요구한다. 따라서 위 논문에서는 generator의 크기를 줄이는 것을 제안한다.

![Untitled](/assets/ELECTRA%20Pre-Training%20Text%20Encoders%20as%20Discriminato%20f10f4341ebab44d597730ecc4fb4260d/Untitled%205.png)

Generator 크기를 discriminator 크기의 1/4 - 1/2로 하는 것이 성능이 제일 좋았다. Generator를 너무 strong하게 하면 discriminator 입장에서 학습하기 너무 어렵다. 그리고 각 token이 실제 token인지 아닌지 판별하는 작업에서보다 generator를 만들 때 parameter가 더 많이 사용될 수 있다.

<br>
<br>

paper

[https://arxiv.org/abs/2003.10555](https://arxiv.org/abs/2003.10555)