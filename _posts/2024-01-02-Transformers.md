---
layout: post
title: "[트랜스포머를 활용한 자연어 처리] Chapter 3: 트랜스포머 파헤치기"
category: "NLP"
date: 2024-01-02
---


Status: Done
작성일: January 2, 2024 8:58 AM

<br>
<br>

**트랜스포머 아키텍처**

- 인코더: 입력 토큰의 시퀀스를 **은닉 상태(hidden state)**라 부르는 임베팅 벡터의 시퀀스로 변환
    - 입력 텍스트를 토큰화하고 토큰 임베딩으로 변환
- 디코더: 인코더의 은닉 상태를 사용해 출력 토큰의 시퀀스를 한 번에 하나씩 생성
    - EOS(end-of-sequence) 토큰이 생성될 때까지 반복
    - 디코더가 앞에서 예측한 토큰과 인코더의 출력을 입력으로 사용
    
<br>
<br>

**인코더**
    
입력 임베딩을 업데이트해 시퀀스의 **문맥** 정보가 인코딩된 표현을 만듦

<br>

- **Self-Attention**
    
    각 토큰은 **고정 차원(BERT에서는 768차원)의 벡터**에 매핑된다. 이때 각 토큰에 대해 고정된 임베딩을 사용하지 않고 전체 시퀀스를 사용해 각 임베딩의 가중 평균을 계산한다. 토큰 임베딩의 시퀀스 $x_1, x_2, ..., x_n$이 주어지면 셀프 어텐션은 새로운 임베딩 시퀀스 $x_1', x_2', ..., x_n'$를 생성한다. 여기서 $x_i'$는 $x_i$의 선형 결합($x_i' = \sum_{j=1}^n w_{ji}x_j$)이다.
    
    ![Untitled](/assets/Chapter%203%20%E1%84%90%E1%85%B3%E1%84%85%E1%85%A2%E1%86%AB%E1%84%89%E1%85%B3%E1%84%91%E1%85%A9%E1%84%86%E1%85%A5%20%E1%84%91%E1%85%A1%E1%84%92%E1%85%A6%E1%84%8E%E1%85%B5%E1%84%80%E1%85%B5%204d96287f5d2945258553260b9b9aed7d/Untitled.png)
    

<br>

- **Scaled Dot-Product Attention**
    
    각 토큰 임베딩을 **쿼리, 키, 값** 이렇게 세 개의 벡터로 투영한다. 그리고 **dot-product**를 사용하여 쿼리 벡터와 키 벡터가 서로 얼마나 관련되어 있는지를 계산한다. 쿼리와 키가 비슷하면 dot-product의 결과가 크다. 그 출력을 **attention score**라고 하며 $n$개의 입력 토큰이 있는 시퀀스의 경우 $n \times n$의 attention score 행렬이 만들어진다. 일반적으로 dot-product는 큰 수를 만들기 때문에 attention score에 scaling factor를 곱해 분산을 정규화하고 softmax 함수를 적용하여 모든 열의 합이 1이 되게 한다. 이를 통해 만들어진 행렬은 **어텐션 가중치**가 담긴다. 어텐션 가중치가 계산되면 이를 값 벡터와 곱하여 토큰 임베딩을 업데이트한다.
    
    ![Untitled](/assets/Chapter%203%20%E1%84%90%E1%85%B3%E1%84%85%E1%85%A2%E1%86%AB%E1%84%89%E1%85%B3%E1%84%91%E1%85%A9%E1%84%86%E1%85%A5%20%E1%84%91%E1%85%A1%E1%84%92%E1%85%A6%E1%84%8E%E1%85%B5%E1%84%80%E1%85%B5%204d96287f5d2945258553260b9b9aed7d/Untitled%201.png)
    
<br>

- **Multi-Head Attention**
    
    여러 벌의 선형 투영을 하는데 이때 각 투영 집합을 어텐션 헤드라고 한다. 한 헤드의 softmax가 유사도의 한 측면에만 초점을 맞추는 경향이 있기 때문이다. 여러 개의 헤드가 있으면 모델은 동시에 여러 측면에 초점을 맞출 수 있다.
    
    ![Untitled](/assets/Chapter%203%20%E1%84%90%E1%85%B3%E1%84%85%E1%85%A2%E1%86%AB%E1%84%89%E1%85%B3%E1%84%91%E1%85%A9%E1%84%86%E1%85%A5%20%E1%84%91%E1%85%A1%E1%84%92%E1%85%A6%E1%84%8E%E1%85%B5%E1%84%80%E1%85%B5%204d96287f5d2945258553260b9b9aed7d/Untitled%202.png)

<br>


- **Feed-Forward Network**
    
    간단하게 두 개의 충으로 이루어진 완전 연결 신경망이다. 이때 전체 임베딩 시퀀스를 하나의 벡터로 처리하지 않고 **각 임베딩을 독립적으로 처리**한다. 보통 첫 번째 층의 크기를 임베딩의 네 배로 하고 activation function으로는 GELU를 사용한다.

<br>

- **Layer Normlization and Skip Connection**
    
    ![Untitled](/assets/Chapter%203%20%E1%84%90%E1%85%B3%E1%84%85%E1%85%A2%E1%86%AB%E1%84%89%E1%85%B3%E1%84%91%E1%85%A9%E1%84%86%E1%85%A5%20%E1%84%91%E1%85%A1%E1%84%92%E1%85%A6%E1%84%8E%E1%85%B5%E1%84%80%E1%85%B5%204d96287f5d2945258553260b9b9aed7d/Untitled%203.png)

<br>

- **위치 임베딩**
    
    토큰 임베딩과 정확히 같은 식으로 동작하지만 입력으로 토큰 ID가 아닌 **위치 인덱스**를 사용한다.
        

<br>
<br>

**디코더**
- Masked Multi-Head Attention
    
    매번 지난 출력과 예측한 현재 토큰만을 사용하여 토큰을 생성한다.

<br>

- **Encoder-Decoder Attention**
    
    디코더의 중간 표현을 쿼리처럼 사용해서 인코터 스택의 출력 키와 값 벡터에 Multi-Head Attention을 수행한다.

<br>