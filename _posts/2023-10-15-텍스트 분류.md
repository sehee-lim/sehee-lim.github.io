---
layout: post
title: "[트랜스포머를 활용한 자연어 처리] Chapter 2: 텍스트 분류"
category: "NLP"
date: 2023-10-15
---


Status: Done

작성일: October 15, 2023 1:31 PM

<br>
<br>

**트랜스포머 모델을 훈련하는 과정**

1. 데이터셋: 데이터셋 로드하고 전처리하기
2. 토크나이저: 입력 텍스트를 토큰화하기
3. 트랜스포머스: 모델을 로드, 훈련, 추론하기
4. 데이터셋: 측정 도구를 로드하고 모델을 평가하기


<br>
<br>



**데이터 다운로드**: load_dataset

```python
from datasets import load_dataset

# emotion 데이터셋 로드
emotions = load_dataset("emotion")
```

```python
# 분류 label 확인할 수 있음
emotions["train"].features
```

⚠️ 이때 토큰의 길이 maximum이 모델의 최대 문맥 크기보다 긴지 확인함


<br>
<br>



**토큰화하기**: tokenizer

1. 문자 토큰화: 각 문자를 정수로 변환
    - 철자 오류나 희귀한 단어를 처리하는 데 유용
    - 단어와 같은 언어 구조를 학습해야 함 (상당량의 계산, 메모리, 데이터 필요)
2. 단어 토큰화: 텍스트를 단어로 분할하고 각 단어를 정수로 변환
    - 어휘사전의 크기가 너무 커짐 (parameter 너무 많아짐)
    - 드물게 등장하는 단어를 UNK 토큰으로 처리하면서 중요한 정보를 일부 잃음
3. 부분단어 토큰화: 자주 등장하는 단어는 고유한 항목으로 유지하고, 드물게 등장하는 단어는 더 작은 단위로 나눔

```python
from transformers import AutoTokenizer   # 사전 훈련된 모델에 연관된 토크나이저 로드

model_ckpt = "distilbert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_ckpt)

text = "Tokenizing text is a core task of NLP."
encoded_text = tokenizer(text)   # text를 input_id로
tokens = tokenizer.convert_ids_to_tokens(encoded_text.input_ids)   # input_id를 token로
tokenizer.convert_tokens_to_string(tokens)   # token을 문자열로
```

```python
# 모델의 최대 문맥 크기 확인
tokenizer.model_max_length
```



<br>



**전체 데이터셋 토큰화하기**

```python
def tokenize(batch):
    return tokenizer(batch["text"], padding = True, truncation = True)
# padding = True: batch["text"에서 가장 긴 샘플 크기에 맞춰 샘플을 0으로 padding
# truncation = True: 모델의 최대 문맥 크기에 맞춰 샘플 잘라냄

emotions_encoded = emotions.map(tokenize, batched = True, batch_size = None)
# batched = True: 토큰화 작업을 batch 단위로 수행하도록 지시
# batch_size = None: 전체 데이터가 하나의 batch가 됨
```

<br>
<br>



**모델 훈련하기**: 은닉 상태를 특성으로 사용해 모델 훈련

```python
from transformers import AutoModel

model_ckpt = "distilbert-base-uncased"
# CPU에서 실행되면 속도가 느려질 수 있음
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = AutoModel.from_pretrained(model_ckpt).to(device)

def extract_hidden_states(batch):
    # GPU로 옮김
    inputs = {k:v.to(device) for k, v in batch.items() if k in tokenizer.model_input_names}

    with torch.no_grad():
        # 마지막 은닉 상태 추출
        last_hidden_state = model(**inputs).last_hidden_state

    # 다시 CPU로 가져옴
    return {"hidden_state": last_hidden_state[:, 0].cpu().numpy()}

# 포맷을 torch로
emotions_encoded.set_format("torch",
                            columns = ["input_ids", "attention_mask", "label"])

emotions_hidden = emotions_encoded.map(extract_hidden_states, batched = True)
# batch_size = 1000 (default)

# 특성 행렬 만들기
import numpy as np

X_train = np.array(emotions_hidden["train"]["hidden_state"])
X_valid = np.array(emotions_hidden["validation"]["hidden_state"])
y_train = np.array(emotions_hidden["train"]["label"])
y_valid = np.array(emotions_hidden["validation"]["label"])

# 로지스틱 회귀모델
from sklearn.linear_model import LogisticRegression

lr_clf = LogisticRegression(max_iter = 3000)
lr_clf.fit(X_train, y_train)
lr_clf.score(X_train, y_train)
```

<br>



**성능 확인**

```python
from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix

def plot_confusion_matrix(y_preds, y_true, labels):
    cm = confusion_matrix(y_true, y_preds, normalize = "true")
    fig, ax = plt.subplots(figsize = (6, 6))
    disp = ConfusionMatrixDisplay(confusion_matrix = cm, display_labels = labels)
    disp.plot(cmap = "Blues", values_format = ".2f", ax = ax, colorbar = False)
    plt.show()

y_preds = lr_clf.predict(X_valid)
plot_confusion_matrix(y_preds, y_valid, labels)
```

<br>
<br>


**모델 훈련하기**: 트랜스포머 미세 튜닝하기

```python
from transformers import AutoModelForSequenceClassification

num_labels = 6
# 사전 훈련된 모델 로드
model = AutoModelForSequenceClassification.from_pretrained(model_ckpt, num_labels = num_labels).to(device)

from transformers import Trainer, TrainingArguments
from sklearn.metrics import accuracy_score, f1_score

# 성능 측정하기 위한 함수
def compute_metrics(pred):
    labels = pred.label_ids
    preds = pred.predictions.argmax(-1)
    f1 = f1_score(labels, preds, average = "weighted")
    acc = accuracy_score(labels, preds)
    return {"accuracy": acc, "f1": f1}

# 훈련 파라미터 정의
batch_size = 64   # 한 번에 training하는 input의 수
logging_steps = len(emotions_encoded["train"]) // batch_size
model_name = f"{model_ckpt}-finetuned-emotion"   # 저장될 모델의 이름
training_args = TrainingArguments(output_dir = model_name,   # 훈련 과정에서 생성된 부산물 저장
                                  num_train_epochs = 2,
                                  learning_rate = 2e-5,
                                  per_device_train_batch_size = batch_size,
                                  per_device_eval_batch_size = batch_size,
                                  weight_decay = 0.01,   # 모델의 파라미터(weights)가 너무 큰 값을 가지지 않도록 제한
                                  evaluation_strategy = "epoch",   # 언제 모델을 평가할지 설정 (각 epoch 끝마다 평가)
                                  disable_tqdm = False,   # 진행률 바 표시 안함
                                  logging_steps = logging_steps,   # 얼마나 자주 로그 출력할지 설정
                                  push_to_hub = True,   # 훈련 완료 후 모델을 HuggingFace의 Model Hub에 푸시
                                  save_strategy = "epoch",   # 언제 모델을 저장할지 설정 (각 epoch 끝마다 저장)
                                  load_best_model_at_end = True,   # 훈련이 끝날 때 가장 좋은 성능을 보인 모델 자동으로 로드
                                  log_level = "error"
                                  )

trainer = Trainer(model = model,
                  args = training_args,
                  compute_metrics = compute_metrics,
                  train_dataset = emotions_encoded['train'],
                  eval_dataset = emotions_encoded["validation"],
                  tokenizer = tokenizer)
# training
trainer.train()
```

<br>

**성능 확인**

```python
preds_output = trainer.predict(emotions_encoded['validation'])
preds_output.metrics

y_preds = np.argmax(preds_output.predictions, axis = 1)
plot_confusion_matrix(y_preds, y_valid, labels)
```

<br>



**Loss 구하기**

```python
from torch.nn.functional import cross_entropy

def forward_pass_with_label(batch):
    # GPU로 옮김
    inputs = {k:v.to(device) for k, v in batch.items() if k in tokenizer.model_input_names}
    
    with torch.no_grad():
        output = model(**inputs)
        pred_label = torch.argmax(output.logits, axis = -1)
        # loss 계산
        loss = cross_entropy(output.logits, batch["label"].to(device), reduction = "none")

    # 다시 CPU로 가져옴
    return {"loss": loss.cpu().numpy(), "predicted_label": pred_label.cpu().numpy()}

# 포맷을 torch로
emotions_encoded.set_format("torch", columns = ["input_ids", "attention_mask", "label"])
emotions_encoded["validation"] = emotions_encoded["validation"].map(
    forward_pass_with_label, batched = True, batch_size = 16
)
```