---
layout: post
title: "Chapter 1: 트랜스포머 소개"
date: 2023-10-13
---



Status: Done

작성일: October 13, 2023 8:12 PM

**인코더와 디코더**

- 인코더: 입력 시퀀스의 정보를 마지막 은닉 상태라는 숫자로 인코딩함
- 디코더: 마지막 은닉 상태가 디코더로 전달되면 디코더는 출력 시퀀스를 생성함

**RNN의 문제점**: 정보 병목

- 디코더는 인코더의 마지막 은닉 상태만을 참조해 출력하기 때문에 마지막 은닉 상태에 전체 입력 시퀀스의 의미가 담겨있어야 함
- 시퀀스가 긴 경우 압축하기 힘들고 시작 부분의 정보가 손실될 가능성이 큼

▶️ **Attention**: 디코더가 인코더의 모든 은닉 상태에 접근해 병목을 제거

- 디코더가 어떤 은닉 상태를 먼저 사용할지 우선순위를 정하는 mechanism
- Attention 기반 모델은 각 t마다 가장 많이 관련된 입력 토큰에 초점을 맞춤
- Transformer는 인코더와 디코더 모두에 self-attention 사용

**ULMFiT**: 자연어 처리에서 사용되는 전이 학습 접근 방식

1. **사전 훈련**: 위키피디아와 같은 대규모 말뭉치를 사용하여 언어 모델을 사전 훈련함, 목표는 이전 단어를 바탕으로 다음 단어를 예측하는 것 (언어 모델링_Language Modeling)
2. **도메인 적응**: 대규모 말뭉치에서 사전 훈련 후 도메인 내 말뭉치에 적응시킴, 이때 특정 작업과 관련된 문맥과 특성을 학습함
3. **미세 튜닝**: 타깃 작업을 위한 분류 층과 함께 미세 튜닝

연구실마다 서로 호환되지 않는 프레임워크(PyTorch, TensorFlow 등)를 사용해 모델을 release하여 NLP 기술자들이 자신의 application에 적용하기 어려웠음

▶️ **Hugging Face Transformers**가 release되면서 단일화된 API가 구축됨

- HuggingFace 허브: 모델, 데이터셋, 평가 지표 등 다양한 NLP 관련 자원들을 공유하는 저장소
- HuggingFace Tokenizer: 다양한 토큰화 전략 제공
- HuggingFace Dataset: 자연어 처리에 사용될 수 있는 다양한 데이터셋과 평가 지표 제공
- HuggingFace Acclerate: 모델의 훈련을 가속화하기 위한 라이브러리

**트랜스포머 application**

1. 텍스트 분류: POSITIVE와 NEGATIVE 중 하나의 레이블 반환
2. 개체명 인식(NER): 텍스트에서 개체명을 감지하고 ORG(조직), LOC(위치), PER(사람), misc(그 외) 중 하나로 할당
3. 질문 답변(quesetion answering): 텍스트와 질문을 모델에 전달하면 모델은 답변과 함께 답변이 위치한 인덱스 처음과 끝을 반환
4. 요약
5. 번역
6. 텍스트 생성